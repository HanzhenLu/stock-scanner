import pandas as pd
import time
from typing import Optional

from app.logger import logger
from app.utils.sse_manager import StreamingSender
from app.utils.config import GenerationConfig
from app.services.prompt_builder import build_enhanced_ai_analysis_prompt, build_K_graph_table_prompt, build_news_section, \
                                        build_news_summary_prompt, build_value_prompt

def generate_ai_analysis(analysis_data:dict, generation_config:GenerationConfig,
                         enable_streaming:bool=False, streamer:StreamingSender=None) -> str:
    """ç”ŸæˆAIå¢å¼ºåˆ†æ - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        logger.info("ğŸ¤– å¼€å§‹AIæ·±åº¦åˆ†æ...")
        
        stock_code = analysis_data['stock_code']
        stock_name = analysis_data['stock_name']
        scores = analysis_data['scores']
        technical_analysis = analysis_data['technical_analysis']
        fundamental_data = analysis_data['fundamental_data']
        sentiment_analysis = analysis_data['sentiment_analysis']
        price_info = analysis_data['price_info']
        
        no_thinking_config = generation_config.model_copy()
        no_thinking_config.extra_parm = {"chat_template_kwargs": {"enable_thinking": False}}
        K_graph_conclusion = k_graph_analysis(stock_name, analysis_data['k_graph_table'], no_thinking_config)
        news_summary = news_summarize(stock_name, sentiment_analysis, no_thinking_config)
        value_analysis = value_analyze(stock_code, stock_name, fundamental_data, price_info, no_thinking_config, streamer)
        
        # æ„å»ºå¢å¼ºç‰ˆAIåˆ†ææç¤ºè¯
        prompt = build_enhanced_ai_analysis_prompt(
            stock_code, stock_name, scores, technical_analysis, 
            fundamental_data, news_summary, price_info, K_graph_conclusion, value_analysis,
            analysis_data["avg_price"], analysis_data["position_percent"]
        )
        streamer.send_prompt("llm-prompt", prompt)
        
        # è®¾ç½®AIæµå¼å†…å®¹å¤„ç†
        ai_content_buffer = ""
        
        def ai_stream_callback(content):
            """AIæµå¼å†…å®¹å›è°ƒ"""
            nonlocal ai_content_buffer
            ai_content_buffer += content
            # å®æ—¶å‘é€AIæµå¼å†…å®¹
            streamer.send_ai_stream(content)
        
        # è°ƒç”¨AI APIï¼ˆæ”¯æŒæµå¼ï¼‰
        ai_response = _call_ai_api(prompt, generation_config, enable_streaming, ai_stream_callback)
        
        if ai_response:
            logger.info("âœ… AIæ·±åº¦åˆ†æå®Œæˆ")
            return ai_response
        else:
            logger.warning("âš ï¸ AI APIä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜çº§åˆ†ææ¨¡å¼")
            return None
            
    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥: {e}")
        return None
    
def k_graph_analysis(stock_name:str, price_data:pd.DataFrame, generation_config:GenerationConfig) -> str:
    prompt = build_K_graph_table_prompt(stock_name, price_data)
    ai_response = _call_ai_api(prompt, generation_config)
    if ai_response:
        logger.info("âœ… K graphè¡¨æ ¼è¯»å–å®Œæˆ")
        return ai_response
    else:
        logger.warning("âš ï¸ K graphè¡¨æ ¼è¯»å–å¤±è´¥")
        return None
    
def news_summarize(stock_name:str, sentiment_analysis:dict, generation_config:GenerationConfig) -> str:
    news_text = build_news_section(
        sentiment_analysis.get('company_news', []),
        sentiment_analysis.get('research_reports', [])
    )
    prompt = build_news_summary_prompt(stock_name, news_text)
    ai_response = _call_ai_api(prompt, generation_config)
    if ai_response:
        logger.info("âœ… æ–°é—»æ€»ç»“å®Œæˆ")
        return ai_response
    else:
        logger.warning("âš ï¸ æ–°é—»æ€»ç»“å¤±è´¥")
        return None
    
def value_analyze(stock_code:str, stock_name:str, fundamental_data:dict, price_info:dict, generation_config:GenerationConfig, streamer:StreamingSender) -> str:
    prompt = build_value_prompt(stock_code, stock_name, fundamental_data, price_info)
    streamer.send_prompt("value-prompt", prompt)
    ai_response = _call_ai_api(prompt, generation_config)
    if ai_response:
        logger.info("âœ… ä»·å€¼åˆ†æå®Œæˆ")
        return ai_response
    else:
        logger.warning("âš ï¸ ä»·å€¼åˆ†æå¤±è´¥")
        return None

def _call_ai_api(prompt: str, generation_config: GenerationConfig, 
                 enable_streaming: bool = False, stream_callback = None) -> Optional[str]:
    """è°ƒç”¨AI API - æ”¯æŒæµå¼è¾“å‡ºï¼Œå¤±è´¥æ—¶æœ€å¤šé‡è¯•2æ¬¡ï¼Œå…±å°è¯•3æ¬¡"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if generation_config.server_name == 'openai':
                result = _call_openai_api(prompt, generation_config, enable_streaming, stream_callback)
                if result is not None:
                    return result

            elif generation_config.server_name == 'anthropic':
                result = _call_claude_api(prompt, generation_config, enable_streaming, stream_callback)
                if result is not None:
                    return result
                
            elif generation_config.server_name == 'zhipu':
                result = _call_zhipu_api(prompt, generation_config, enable_streaming, stream_callback)
                if result is not None:
                    return result

            # å¦‚æœè¿”å›äº† Noneï¼Œä¹Ÿè§†ä¸ºå¤±è´¥ï¼Œè¿›è¡Œé‡è¯•
            logger.warning(f"API è°ƒç”¨è¿”å› Noneï¼Œç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•...")

        except Exception as e:
            logger.error(f"AI APIè°ƒç”¨å¼‚å¸¸ï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰: {e}")
            time.sleep(1)
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿å¯é€‰ï¼‰
        if attempt < max_retries - 1:
            time.sleep(2 ** attempt)  # å¯é€‰ï¼šåŠ å…¥æŒ‡æ•°é€€é¿
        else:
            logger.error("å·²å°è¯• 3 æ¬¡ï¼Œå…¨éƒ¨å¤±è´¥ï¼Œæ”¾å¼ƒé‡è¯•ã€‚")
    
    return None

def _call_openai_api(prompt:str, generation_config:GenerationConfig, 
                     enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨OpenAI API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import openai
        
        logger.info(f"æ­£åœ¨è°ƒç”¨OpenAI {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # æ£€æµ‹OpenAIåº“ç‰ˆæœ¬å¹¶ä½¿ç”¨ç›¸åº”çš„API
        try:
            client = openai.OpenAI(api_key=generation_config.api_key)
            if generation_config.api_base_url:
                client.base_url = generation_config.api_base_url
            
            if enable_streaming and stream_callback:
                # æµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    max_tokens=generation_config.max_tokens,
                    temperature=generation_config.temperature,
                    stream=True,
                    extra_body=generation_config.extra_parm
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        # å‘é€æµå¼å†…å®¹
                        if stream_callback:
                            stream_callback(content)
                
                return full_response
            else:
                # éæµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    max_tokens=generation_config.max_tokens,
                    temperature=generation_config.temperature,
                    extra_body=generation_config.extra_parm
                )
                return response.choices[0].message.content
                
        except Exception as api_error:
            logger.error(f"OpenAI APIè°ƒç”¨é”™è¯¯: {api_error}")
            return None
            
    except ImportError:
        logger.error("OpenAIåº“æœªå®‰è£…")
        return None
    except Exception as e:
        logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def _call_claude_api(prompt:str, generation_config:GenerationConfig,
                     enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨Claude API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import anthropic
        
        client = anthropic.Anthropic(api_key=generation_config.api_key)
        
        logger.info(f"æ­£åœ¨è°ƒç”¨Claude {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        if enable_streaming and stream_callback:
            # æµå¼è°ƒç”¨
            with client.messages.stream(
                model=generation_config.model_name,
                max_tokens=generation_config.max_tokens,
                messages=messages
            ) as stream:
                full_response = ""
                for text in stream.text_stream:
                    full_response += text
                    # å‘é€æµå¼å†…å®¹
                    if stream_callback:
                        stream_callback(text)
            
            return full_response
        else:
            # éæµå¼è°ƒç”¨
            response = client.messages.create(
                model=generation_config.model_name,
                max_tokens=generation_config.max_tokens,
                messages=messages
            )
            
            return response.content[0].text
        
    except Exception as e:
        logger.error(f"Claude APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def _call_zhipu_api(prompt:str, generation_config:GenerationConfig, enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨æ™ºè°±AI API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import zhipuai
        
        client = zhipuai.ZhipuAI(api_key=generation_config.api_key)
        
        logger.info(f"æ­£åœ¨è°ƒç”¨æ™ºè°±AI {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            if enable_streaming and stream_callback:
                # æµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    temperature=generation_config.temperature,
                    max_tokens=generation_config.max_tokens,
                    stream=True
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        # å‘é€æµå¼å†…å®¹
                        if stream_callback:
                            stream_callback(content)
                
                return full_response
            else:
                # éæµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    temperature=generation_config.temperature,
                    max_tokens=generation_config.max_tokens
                )
                return response.choices[0].message.content
                
        except Exception as api_error:
            logger.error(f"æ™ºè°±AI APIè°ƒç”¨é”™è¯¯: {api_error}")
            return None

    except ImportError:
        logger.error("æ™ºè°±AIåº“æœªå®‰è£…")
        return None
        
    except Exception as e:
        logger.error(f"æ™ºè°±AI APIè°ƒç”¨å¤±è´¥: {e}")
        return None