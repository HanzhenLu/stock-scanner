import pandas as pd

from app.logger import logger
from app.utils.config import GenerationConfig
from app.services.prompt_builder import build_enhanced_ai_analysis_prompt, build_K_graph_table_prompt

def generate_ai_analysis(analysis_data:dict, generation_config:GenerationConfig,
                         enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """ç”ŸæˆAIå¢å¼ºåˆ†æ - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        logger.info("ğŸ¤– å¼€å§‹AIæ·±åº¦åˆ†æ...")
        
        stock_code = analysis_data['stock_code']
        stock_name = analysis_data['stock_name']
        scores = analysis_data['scores']
        technical_analysis = analysis_data['technical_analysis']
        fundamental_data = analysis_data['fundamental_data']
        sentiment_analysis = analysis_data['sentiment_analysis']
        price_info = analysis_data['price_info']
        
        K_graph_conclusion = k_graph_analysis(analysis_data['k_graph_table'], generation_config)
        
        # æ„å»ºå¢å¼ºç‰ˆAIåˆ†ææç¤ºè¯
        prompt = build_enhanced_ai_analysis_prompt(
            stock_code, stock_name, scores, technical_analysis, 
            fundamental_data, sentiment_analysis, price_info, K_graph_conclusion
        )
        
        # è°ƒç”¨AI APIï¼ˆæ”¯æŒæµå¼ï¼‰
        ai_response = _call_ai_api(prompt, generation_config, enable_streaming, stream_callback)
        
        if ai_response:
            logger.info("âœ… AIæ·±åº¦åˆ†æå®Œæˆ")
            return ai_response
        else:
            logger.warning("âš ï¸ AI APIä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜çº§åˆ†ææ¨¡å¼")
            return None
            
    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥: {e}")
        return None
    
def k_graph_analysis(price_data:pd.DataFrame, generation_config:GenerationConfig) -> str:
    prompt = build_K_graph_table_prompt(price_data)
    no_thinking_config = generation_config.model_copy()
    no_thinking_config.extra_parm = {"chat_template_kwargs": {"enable_thinking": False}}
    ai_response = _call_ai_api(prompt, no_thinking_config)
    if ai_response:
        logger.info("âœ… K graphè¡¨æ ¼è¯»å–å®Œæˆ")
        return ai_response
    else:
        logger.warning("âš ï¸ K graphè¡¨æ ¼è¯»å–å¤±è´¥")
        return None
    
def _call_ai_api(prompt:str, generation_config:GenerationConfig, 
                 enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨AI API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        if generation_config.server_name == 'openai':
            result = _call_openai_api(prompt, generation_config, enable_streaming, stream_callback)
            if result:
                return result
        
        elif generation_config.server_name == 'anthropic':
            result = _call_claude_api(prompt, generation_config, enable_streaming, stream_callback)
            if result:
                return result
                
        elif generation_config.server_name == 'zhipu':
            result = _call_zhipu_api(prompt, generation_config, enable_streaming, stream_callback)
            if result:
                return result
        
        return None
            
    except Exception as e:
        logger.error(f"AI APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def _call_openai_api(prompt:str, generation_config:GenerationConfig, 
                     enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨OpenAI API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import openai
        
        logger.info(f"æ­£åœ¨è°ƒç”¨OpenAI {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        # æ£€æµ‹OpenAIåº“ç‰ˆæœ¬å¹¶ä½¿ç”¨ç›¸åº”çš„API
        try:
            client = openai.OpenAI(api_key=generation_config.api_key)
            if generation_config.api_base_url:
                client.base_url = generation_config.api_base_url
            
            if enable_streaming and stream_callback:
                # æµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    max_tokens=generation_config.max_tokens,
                    temperature=generation_config.temperature,
                    stream=True,
                    extra_body=generation_config.extra_parm
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        # å‘é€æµå¼å†…å®¹
                        if stream_callback:
                            stream_callback(content)
                
                return full_response
            else:
                # éæµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    max_tokens=generation_config.max_tokens,
                    temperature=generation_config.temperature,
                    extra_body=generation_config.extra_parm
                )
                return response.choices[0].message.content
                
        except Exception as api_error:
            logger.error(f"OpenAI APIè°ƒç”¨é”™è¯¯: {api_error}")
            return None
            
    except ImportError:
        logger.error("OpenAIåº“æœªå®‰è£…")
        return None
    except Exception as e:
        logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def _call_claude_api(prompt:str, generation_config:GenerationConfig,
                     enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨Claude API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import anthropic
        
        client = anthropic.Anthropic(api_key=generation_config.api_key)
        
        logger.info(f"æ­£åœ¨è°ƒç”¨Claude {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        if enable_streaming and stream_callback:
            # æµå¼è°ƒç”¨
            with client.messages.stream(
                model=generation_config.model_name,
                max_tokens=generation_config.max_tokens,
                messages=messages
            ) as stream:
                full_response = ""
                for text in stream.text_stream:
                    full_response += text
                    # å‘é€æµå¼å†…å®¹
                    if stream_callback:
                        stream_callback(text)
            
            return full_response
        else:
            # éæµå¼è°ƒç”¨
            response = client.messages.create(
                model=generation_config.model_name,
                max_tokens=generation_config.max_tokens,
                messages=messages
            )
            
            return response.content[0].text
        
    except Exception as e:
        logger.error(f"Claude APIè°ƒç”¨å¤±è´¥: {e}")
        return None

def _call_zhipu_api(prompt:str, generation_config:GenerationConfig, enable_streaming:bool=False, stream_callback:bool=None) -> str:
    """è°ƒç”¨æ™ºè°±AI API - æ”¯æŒæµå¼è¾“å‡º"""
    try:
        import zhipuai
        
        client = zhipuai.ZhipuAI(api_key=generation_config.api_key)
        
        logger.info(f"æ­£åœ¨è°ƒç”¨æ™ºè°±AI {generation_config.model_name} è¿›è¡Œæ·±åº¦åˆ†æ...")
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è‚¡ç¥¨åˆ†æå¸ˆï¼Œå…·æœ‰ä¸°å¯Œçš„å¸‚åœºç»éªŒå’Œæ·±åšçš„é‡‘èçŸ¥è¯†ã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€æœ‰æ·±åº¦çš„è‚¡ç¥¨åˆ†æã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            if enable_streaming and stream_callback:
                # æµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    temperature=generation_config.temperature,
                    max_tokens=generation_config.max_tokens,
                    stream=True
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        # å‘é€æµå¼å†…å®¹
                        if stream_callback:
                            stream_callback(content)
                
                return full_response
            else:
                # éæµå¼è°ƒç”¨
                response = client.chat.completions.create(
                    model=generation_config.model_name,
                    messages=messages,
                    temperature=generation_config.temperature,
                    max_tokens=generation_config.max_tokens
                )
                return response.choices[0].message.content
                
        except Exception as api_error:
            logger.error(f"æ™ºè°±AI APIè°ƒç”¨é”™è¯¯: {api_error}")
            return None

    except ImportError:
        logger.error("æ™ºè°±AIåº“æœªå®‰è£…")
        return None
        
    except Exception as e:
        logger.error(f"æ™ºè°±AI APIè°ƒç”¨å¤±è´¥: {e}")
        return None