import pandas as pd
import akshare as ak
import math
import time
from datetime import datetime, timedelta

from app.logger import logger
from app.utils.config import load_config
from app.utils.financial_utils import (get_price_info, calculate_technical_indicators, get_K_graph_table)
from app.utils.sse_manager import StreamingSender
from app.container import sse_manager
from app.services.ai_client import generate_ai_analysis, news_summarize, k_graph_analysis, value_analyze
from app.services.prompt_builder import build_value_prompt

class WebStockAnalyzer:
    """Webç‰ˆå¢å¼ºè‚¡ç¥¨åˆ†æå™¨"""
    
    def __init__(self, config_file='config.json'):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config_file = config_file
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = load_config(self.config_file)
        
        # ç¼“å­˜é…ç½®
        self.price_cache_duration = timedelta(hours=self.config.cache.price_hours)
        self.fundamental_cache_duration = timedelta(hours=self.config.cache.fundamental_hours)
        self.news_cache_duration = timedelta(hours=self.config.cache.news_hours)
        
        self.price_cache = {}
        self.fundamental_cache = {}
        self.news_cache = {}
        
        # æƒé‡é…ç½®ï¼Œ é‡æ–°å½’ä¸€åŒ–
        weights_sum = self.config.analysis_weights.technical + self.config.analysis_weights.fundamental + self.config.analysis_weights.sentiment
        if weights_sum != 1:
            logger.info("é‡æ–°å½’ä¸€åŒ–")
            self.config.analysis_weights.technical /= weights_sum
            self.config.analysis_weights.fundamental /= weights_sum
            self.config.analysis_weights.sentiment /= weights_sum
        
        logger.info("Webç‰ˆè‚¡ç¥¨åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self._log_config_status()

    def _log_config_status(self):
        """è®°å½•é…ç½®çŠ¶æ€"""
        logger.info("=== Webç‰ˆç³»ç»Ÿé…ç½®çŠ¶æ€===")
        
        # æ£€æŸ¥AIçŠ¶æ€
        logger.info(f"ğŸ¤– ä½¿ç”¨AI: {self.config.generation.server_name}:{self.config.generation.model_name}")
        logger.info(f"ğŸ¯ ä½¿ç”¨url: {self.config.generation.api_base_url}")
        
        if not self.config.generation.api_key:
            logger.warning("âš ï¸ æœªæä¾›api keys")
        
        logger.info(f"ğŸ“Š è´¢åŠ¡æŒ‡æ ‡æ•°é‡: {self.config.analysis_params.financial_indicators_count}")
        logger.info(f"ğŸ“° æœ€å¤§æ–°é—»æ•°é‡: {self.config.analysis_params.max_news_count}")
        logger.info(f"ğŸ“ˆ æŠ€æœ¯åˆ†æå‘¨æœŸ: {self.config.analysis_params.technical_period_days} å¤©")
        
        # æ£€æŸ¥Webé‰´æƒé…ç½®
        if self.config.web_auth.enabled:
            logger.info(f"ğŸ” Webé‰´æƒ: å·²å¯ç”¨")
        else:
            logger.info(f"ğŸ”“ Webé‰´æƒ: æœªå¯ç”¨")
        
        logger.info("=" * 40)

    def get_stock_data(self, stock_code:str):
        """è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®"""
        if stock_code in self.price_cache:
            cache_time, data = self.price_cache[stock_code]
            if datetime.now() - cache_time < self.price_cache_duration:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„ä»·æ ¼æ•°æ®: {stock_code}")
                return data
        
        try:
            end_date = datetime.now().strftime('%Y%m%d')
            # ä½¿ç”¨ç”¨æˆ·é…ç½®çš„æŠ€æœ¯åˆ†æå‘¨æœŸ
            days = self.config.analysis_params.technical_period_days
            start_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
            
            logger.info(f"æ­£åœ¨è·å– {stock_code} çš„å†å²æ•°æ® (è¿‡å»{days}å¤©)...")
            
            stock_data = ak.stock_zh_a_hist(
                symbol=stock_code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"
            )
            
            if stock_data.empty:
                raise ValueError(f"æ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„æ•°æ®")
            
            actual_columns = len(stock_data.columns)
            logger.info(f"è·å–åˆ° {actual_columns} åˆ—æ•°æ®ï¼Œåˆ—å: {list(stock_data.columns)}")
            
            # æ ¹æ®å®é™…è¿”å›çš„åˆ—æ•°è¿›è¡Œæ˜ å°„
            if actual_columns == 13:  # åŒ…å«codeåˆ—çš„å®Œæ•´æ ¼å¼
                standard_columns = ['date', 'code', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_pct', 'change_amount', 'turnover_rate', 'extra']
            elif actual_columns == 12:  # åŒ…å«codeåˆ—
                standard_columns = ['date', 'code', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_pct', 'change_amount', 'turnover_rate']
            elif actual_columns == 11:  # ä¸åŒ…å«codeåˆ—çš„æ ‡å‡†æ ¼å¼
                standard_columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_pct', 'change_amount', 'turnover_rate']
            elif actual_columns == 10:  # ç®€åŒ–æ ¼å¼
                standard_columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_pct', 'change_amount']
            else:
                raise ValueError(f"è‚¡ç¥¨ {stock_code} è·å–æ•°æ®æ ¼å¼é”™è¯¯ åˆ—æ•°ä»…æœ‰ {actual_columns}")
            
            # åˆ›å»ºåˆ—åæ˜ å°„
            column_mapping = dict(zip(stock_data.columns, standard_columns))
            stock_data = stock_data.rename(columns=column_mapping)
            
            logger.info(f"åˆ—åæ˜ å°„å®Œæˆ: {column_mapping}")
            
            # å¤„ç†æ—¥æœŸåˆ—
            try:
                stock_data['date'] = pd.to_datetime(stock_data['date'])
                stock_data = stock_data.set_index('date')
            except Exception as e:
                logger.warning(f"æ—¥æœŸå¤„ç†å¤±è´¥: {e}")
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
            numeric_columns = ['open', 'close', 'high', 'low', 'volume']
            for col in numeric_columns:
                if col in stock_data.columns:
                    try:
                        stock_data[col] = pd.to_numeric(stock_data[col], errors='coerce')
                    except:
                        pass
            
            # éªŒè¯æ•°æ®è´¨é‡
            if 'close' in stock_data.columns:
                latest_close = stock_data['close'].iloc[-1]
                latest_open = stock_data['open'].iloc[-1] if 'open' in stock_data.columns else 0
                logger.info(f"âœ“ æ•°æ®éªŒè¯ - æœ€æ–°æ”¶ç›˜ä»·: {latest_close}, æœ€æ–°å¼€ç›˜ä»·: {latest_open}")
                
                # æ£€æŸ¥æ”¶ç›˜ä»·æ˜¯å¦åˆç†
                if pd.isna(latest_close) or latest_close <= 0:
                    logger.error(f"âŒ æ”¶ç›˜ä»·æ•°æ®å¼‚å¸¸: {latest_close}")
                    raise ValueError(f"è‚¡ç¥¨ {stock_code} çš„æ”¶ç›˜ä»·æ•°æ®å¼‚å¸¸")
            
            # ç¼“å­˜æ•°æ®
            self.price_cache[stock_code] = (datetime.now(), stock_data)
            
            logger.info(f"âœ“ æˆåŠŸè·å– {stock_code} çš„ä»·æ ¼æ•°æ®ï¼Œå…± {len(stock_data)} æ¡è®°å½•")
            logger.info(f"âœ“ æ•°æ®åˆ—: {list(stock_data.columns)}")
            
            return stock_data
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def get_comprehensive_fundamental_data(self, stock_code:str) -> dict:
        """è·å–é¡¹ç»¼åˆè´¢åŠ¡æŒ‡æ ‡æ•°æ®"""
        if stock_code in self.fundamental_cache:
            cache_time, data = self.fundamental_cache[stock_code]
            if datetime.now() - cache_time < self.fundamental_cache_duration:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„åŸºæœ¬é¢æ•°æ®: {stock_code}")
                return data
        
        current_time = datetime.today()
        
        try:
            fundamental_data = {}
            logger.info(f"å¼€å§‹è·å– {stock_code} çš„ç»¼åˆè´¢åŠ¡æŒ‡æ ‡...")
            
            # 1. åŸºæœ¬ä¿¡æ¯
            try:
                logger.info("æ­£åœ¨è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
                stock_info = ak.stock_individual_info_em(symbol=stock_code)
                info_dict = dict(zip(stock_info['item'], stock_info['value']))
                fundamental_data['basic_info'] = info_dict
                logger.info("âœ“ è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è·å–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"è·å–åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
                fundamental_data['basic_info'] = {}
            
            # 2. è¯¦ç»†è´¢åŠ¡æŒ‡æ ‡ - æ ¸å¿ƒæŒ‡æ ‡
            try:
                logger.info("æ­£åœ¨è·å–è¯¦ç»†è´¢åŠ¡æŒ‡æ ‡...")
                
                # è·å–è´¢åŠ¡åˆ†ææŒ‡æ ‡
                financial_analysis_indicator = ak.stock_financial_analysis_indicator(symbol=stock_code, start_year=f"{current_time.year}")
                if not financial_analysis_indicator.empty:
                    latest_financial_analysis_indicator = financial_analysis_indicator.iloc[-1].to_dict()
                
                def safe_isnan(x):
                    try:
                        return math.isnan(x)
                    except:
                        return False
                
                fundamental_data['financial_indicators'] = {
                    k: v for k, v in latest_financial_analysis_indicator.items()
                    if v not in [0, None, 'nan', -1] and not safe_isnan(v)
                }
                logger.info(f"è·å–åˆ°{len(fundamental_data['financial_indicators'].keys())}æ¡è´¢åŠ¡åˆ†ææŒ‡æ ‡")
                
            except Exception as e:
                logger.warning(f"è·å–è´¢åŠ¡æŒ‡æ ‡å¤±è´¥: {e}")
                fundamental_data['financial_indicators'] = {}
            
            # 3. ä¼°å€¼æŒ‡æ ‡
            try:
                logger.info("æ­£åœ¨è·å–ä¼°å€¼æŒ‡æ ‡...")
                valuation_data = ak.stock_value_em(symbol=stock_code)
                if not valuation_data.empty:
                    latest_valuation = valuation_data.iloc[-1].to_dict()
                    # æ¸…ç†ä¼°å€¼æ•°æ®ä¸­çš„NaNå€¼
                    cleaned_valuation = {}
                    for key, value in latest_valuation.items():
                        if pd.isna(value) or (isinstance(value, float) and (math.isnan(value) or math.isinf(value))):
                            cleaned_valuation[key] = None
                        else:
                            cleaned_valuation[key] = value
                    fundamental_data['valuation'] = cleaned_valuation
                    logger.info("âœ“ ä¼°å€¼æŒ‡æ ‡è·å–æˆåŠŸ")
                else:
                    fundamental_data['valuation'] = {}
            except Exception as e:
                logger.warning(f"è·å–ä¼°å€¼æŒ‡æ ‡å¤±è´¥: {e}")
                fundamental_data['valuation'] = {}
            
            # 4. ä¸šç»©é¢„å‘Šå’Œä¸šç»©å¿«æŠ¥
            try:
                logger.info("æ­£åœ¨è·å–ä¸šç»©æŠ¥è¡¨...")
                if current_time.month <= 3:
                    query_time = [f"{current_time.year-1}1231", f"{current_time.year-1}0930", f"{current_time.year-1}0630", f"{current_time.year-1}0331"]
                elif current_time.month <= 6:
                    query_time = [f"{current_time.year}0331", f"{current_time.year-1}1231", f"{current_time.year-1}0930", f"{current_time.year-1}0630"]
                elif current_time.month <= 9:
                    query_time = [f"{current_time.year}0630", f"{current_time.year}0331", f"{current_time.year-1}1231", f"{current_time.year-1}0930"]
                else:
                    query_time = [f"{current_time.year}0930", f"{current_time.year}0630", f"{current_time.year}0331", f"{current_time.year-1}1231"]
                for t in query_time:
                    time.sleep(1)
                    performance_forecast = ak.stock_yjbb_em(t)
                    if stock_code in performance_forecast["è‚¡ç¥¨ä»£ç "].values:
                        break
                if stock_code in performance_forecast["è‚¡ç¥¨ä»£ç "].values:
                    fundamental_data['performance_repo'] = performance_forecast[performance_forecast["è‚¡ç¥¨ä»£ç "] == stock_code].iloc[0].to_dict()
                    logger.info("âœ“ ä¸šç»©æŠ¥è¡¨è·å–æˆåŠŸ")
                else:
                    logger.info("æœªèƒ½æŸ¥æ‰¾åˆ°ä¸šç»©æŠ¥è¡¨")
                    fundamental_data['performance_repo'] = "æœªèƒ½æ‰¾åˆ°ä¸šç»©æŠ¥è¡¨"
            except Exception as e:
                logger.warning(f"è·å–ä¸šç»©æŠ¥è¡¨å¤±è´¥: {e}")
                fundamental_data['performance_repo'] = "æœªèƒ½æ‰¾åˆ°ä¸šç»©æŠ¥è¡¨"
            
            # 5. åˆ†çº¢é…è‚¡ä¿¡æ¯
            try:
                logger.info("æ­£åœ¨è·å–åˆ†çº¢é…è‚¡ä¿¡æ¯...")
                dividend_info = ak.stock_fhps_detail_em(stock_code)
                if not dividend_info.empty:
                    dividend_info_list = []
                    for i in range(min(5, len(dividend_info))):
                        dividend_info_list.append(dividend_info.iloc[-(i+1)].to_dict())
                    fundamental_data['dividend_info'] = dividend_info_list
                    logger.info("âœ“ åˆ†çº¢é…è‚¡ä¿¡æ¯è·å–æˆåŠŸ")
                else:
                    fundamental_data['dividend_info'] = []
            except Exception as e:
                logger.warning(f"è·å–åˆ†çº¢é…è‚¡ä¿¡æ¯å¤±è´¥: {e}")
                fundamental_data['dividend_info'] = []
            
            # 6. è¡Œä¸šåˆ†æ
            try:
                logger.info("æ­£åœ¨è·å–è¡Œä¸šåˆ†ææ•°æ®...")
                industry_analysis = self.get_industry_analysis(fundamental_data['basic_info']["è¡Œä¸š"])
                fundamental_data['industry_analysis'] = industry_analysis
                logger.info("âœ“ è¡Œä¸šåˆ†ææ•°æ®è·å–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"è·å–è¡Œä¸šåˆ†æå¤±è´¥: {e}")
                fundamental_data['industry_analysis'] = {}
            
            # ç¼“å­˜æ•°æ®
            self.fundamental_cache[stock_code] = (datetime.now(), fundamental_data)
            logger.info(f"âœ“ {stock_code} ç»¼åˆåŸºæœ¬é¢æ•°æ®è·å–å®Œæˆå¹¶å·²ç¼“å­˜")
            
            return fundamental_data
            
        except Exception as e:
            logger.error(f"è·å–ç»¼åˆåŸºæœ¬é¢æ•°æ®å¤±è´¥: {str(e)}")
            return {
                'basic_info': {},
                'financial_indicators': {},
                'valuation': {},
                'performance_forecast': [],
                'dividend_info': [],
                'industry_analysis': {}
            }

    def get_industry_analysis(self, industry_name:str) -> dict:
        """è·å–è¡Œä¸šåˆ†ææ•°æ®"""
        try:
            industry_data = {}
            current_time = datetime.today()

            # è·å–è¡Œä¸šä¿¡æ¯
            try:
                industry_info = ak.stock_board_industry_name_em()
                stock_industry_info = industry_info[industry_info["æ¿å—åç§°"] == industry_name].iloc[0].to_dict()
                industry_data['industry_info'] = stock_industry_info
            except Exception as e:
                logger.warning(f"è·å–è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
                industry_data['industry_info'] = {}
            
            try:
                # æœ€è¿‘ 30 å¤©çš„äº¤æ˜“æ—¥æ•°æ®
                start_date = (current_time - timedelta(days=30)).strftime('%Y%m%d')
                stock_data = ak.stock_zh_a_hist(
                    symbol="000001",
                    period="daily",
                    start_date=start_date,
                    end_date=current_time.strftime("%Y%m%d"),
                    adjust="qfq"
                )
                # æœ€è¿‘ä¸¤ä¸ªäº¤æ˜“æ—¥ï¼ŒæŒ‰æ—¥æœŸå‡åºæ’åˆ—
                date_df = stock_data[['æ—¥æœŸ']].sort_values('æ—¥æœŸ').reset_index(drop=True)
                last_trading_day = date_df.iloc[-1]['æ—¥æœŸ']
                previous_trading_day = date_df.iloc[-2]['æ—¥æœŸ']

                # å†³å®šç”¨äºè·å– PE çš„æ—¥æœŸ
                if last_trading_day.strftime('%Y-%m-%d') == current_time.strftime('%Y-%m-%d') and current_time.hour < 17:
                    # ä»Šå¤©äº¤æ˜“æ—¥ä½†æœªæ”¶ç›˜ â†’ ç”¨ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
                    pe_date = previous_trading_day.strftime('%Y%m%d')
                else:
                    # ä»Šå¤©éäº¤æ˜“æ—¥ï¼Œæˆ–å·²æ”¶ç›˜ â†’ ç”¨æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
                    pe_date = last_trading_day.strftime('%Y%m%d')

                # è·å–è¡Œä¸šå¸‚ç›ˆç‡
                industry_pe_info = ak.stock_industry_pe_ratio_cninfo("å›½è¯è¡Œä¸šåˆ†ç±»", pe_date)
                if industry_name not in industry_pe_info["è¡Œä¸šåç§°"].to_list():
                    industry_pe_info = ak.stock_industry_pe_ratio_cninfo("è¯ç›‘ä¼šè¡Œä¸šåˆ†ç±»", pe_date)
                if industry_name in industry_pe_info["è¡Œä¸šåç§°"].to_list():
                    stock_industry_pe_info = industry_pe_info[industry_pe_info["è¡Œä¸šåç§°"] == industry_name].iloc[0].to_dict()
                    industry_data['industry_pe_info'] = stock_industry_pe_info
                else:
                    stock_board_industry_cons_em_df = ak.stock_board_industry_cons_em(symbol=industry_name)
                    industry_data['industry_pe_info'] = {
                        "å¹³å‡æ¢æ‰‹ç‡": round(float(stock_board_industry_cons_em_df["æ¢æ‰‹ç‡"].mean()), 2),
                        "å¹³å‡å¸‚ç›ˆç‡-åŠ¨æ€": round(float(stock_board_industry_cons_em_df["å¸‚ç›ˆç‡-åŠ¨æ€"].mean()), 2),
                        "å¹³å‡å¸‚å‡€ç‡": round(float(stock_board_industry_cons_em_df["å¸‚å‡€ç‡"].mean()), 2)
                    }

            except Exception as e:
                logger.warning(f"è·å–è¡Œä¸šå¸‚ç›ˆç‡å¤±è´¥: {e}")
                industry_data['industry_pe_info'] = {}

            
            return industry_data
            
        except Exception as e:
            logger.warning(f"è¡Œä¸šåˆ†æå¤±è´¥: {e}")
            return {}

    def get_comprehensive_news_data(self, stock_code:str, days:int=15) -> dict:
        """è·å–ç»¼åˆæ–°é—»æ•°æ®ï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰"""
        cache_key = f"{stock_code}_{days}"
        if cache_key in self.news_cache:
            cache_time, data = self.news_cache[cache_key]
            if datetime.now() - cache_time < self.news_cache_duration:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®: {stock_code}")
                return data
        
        logger.info(f"å¼€å§‹è·å– {stock_code} çš„ç»¼åˆæ–°é—»æ•°æ®ï¼ˆæœ€è¿‘{days}å¤©ï¼‰...")
        
        try:
            all_news_data = {
                'company_news': [],
                'research_reports': [],
                'market_sentiment': {},
                'news_summary': {}
            }
            
            # 1. å…¬å¸æ–°é—»
            try:
                logger.info("æ­£åœ¨è·å–å…¬å¸æ–°é—»...")
                company_news = ak.stock_news_em(symbol=stock_code)
                if not company_news.empty:
                    processed_news = []
                    for _, row in company_news.head(50).iterrows():  # å¢åŠ è·å–æ•°é‡
                        news_item = {
                            'title': row.iloc[1],
                            'content': row.iloc[2],
                            'date': row.iloc[3],
                            'source': row.iloc[4],
                            'url': row.iloc[5],
                            'relevance_score': 1.0
                        }
                        processed_news.append(news_item)
                    
                    all_news_data['company_news'] = processed_news
                    logger.info(f"âœ“ è·å–å…¬å¸æ–°é—» {len(processed_news)} æ¡")
            except Exception as e:
                logger.warning(f"è·å–å…¬å¸æ–°é—»å¤±è´¥: {e}")
            
            # 3. ç ”ç©¶æŠ¥å‘Š
            try:
                logger.info("æ­£åœ¨è·å–ç ”ç©¶æŠ¥å‘Š...")
                research_reports = ak.stock_research_report_em(symbol=stock_code)
                if not research_reports.empty:
                    processed_reports = []
                    for _, row in research_reports.head(20).iterrows():  # å¢åŠ è·å–æ•°é‡
                        report = {
                            'title': row.iloc[3],
                            'institution': row.iloc[5],
                            'rating': row.iloc[4],
                            'target_price': row.iloc[7],
                            'date': row.iloc[14],
                            'relevance_score': 0.9
                        }
                        processed_reports.append(report)
                    
                    all_news_data['research_reports'] = processed_reports
                    logger.info(f"âœ“ è·å–ç ”ç©¶æŠ¥å‘Š {len(processed_reports)} æ¡")
            except Exception as e:
                logger.warning(f"è·å–ç ”ç©¶æŠ¥å‘Šå¤±è´¥: {e}")
            
            # 5. æ–°é—»æ‘˜è¦ç»Ÿè®¡
            try:
                total_news = (len(all_news_data['company_news']) + 
                            len(all_news_data['research_reports']))
                
                all_news_data['news_summary'] = {
                    'total_news_count': total_news,
                    'company_news_count': len(all_news_data['company_news']),
                    'research_reports_count': len(all_news_data['research_reports']),
                    'data_freshness': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
            except Exception as e:
                logger.warning(f"ç”Ÿæˆæ–°é—»æ‘˜è¦å¤±è´¥: {e}")
            
            # ç¼“å­˜æ•°æ®
            self.news_cache[cache_key] = (datetime.now(), all_news_data)
            
            logger.info(f"âœ“ ç»¼åˆæ–°é—»æ•°æ®è·å–å®Œæˆï¼Œæ€»è®¡ {all_news_data['news_summary'].get('total_news_count', 0)} æ¡")
            return all_news_data
            
        except Exception as e:
            logger.error(f"è·å–ç»¼åˆæ–°é—»æ•°æ®å¤±è´¥: {str(e)}")
            return {
                'company_news': [],
                'research_reports': [],
                'market_sentiment': {},
                'news_summary': {'total_news_count': 0}
            }

    def calculate_advanced_sentiment_analysis(self, comprehensive_news_data:dict) -> dict:
        """è®¡ç®—é«˜çº§æƒ…ç»ªåˆ†æï¼ˆä¿®æ­£ç‰ˆæœ¬ï¼‰"""
        logger.info("å¼€å§‹é«˜çº§æƒ…ç»ªåˆ†æ...")
        
        try:
            # å‡†å¤‡æ‰€æœ‰æ–°é—»æ–‡æœ¬
            all_texts = []
            
            # æ”¶é›†æ‰€æœ‰æ–°é—»æ–‡æœ¬
            for news in comprehensive_news_data.get('company_news', []):
                text = f"{news.get('title', '')} {news.get('content', '')}"
                all_texts.append({'text': text, 'type': 'company_news', 'weight': 1.0})
            
            for report in comprehensive_news_data.get('research_reports', []):
                text = f"{report.get('title', '')} {report.get('rating', '')}"
                all_texts.append({'text': text, 'type': 'research_report', 'weight': 0.9})
            
            if not all_texts:
                return {
                    'overall_sentiment': -1,
                    'sentiment_by_type': {},
                    'sentiment_trend': 'åˆ†æå¤±è´¥',
                    'confidence_score': -1,
                    'total_analyzed': -1
                }
            
            # æ‰©å±•çš„æƒ…ç»ªè¯å…¸
            positive_words = {
                'ä¸Šæ¶¨', 'æ¶¨åœ', 'åˆ©å¥½', 'çªç ´', 'å¢é•¿', 'ç›ˆåˆ©', 'æ”¶ç›Š', 'å›å‡', 'å¼ºåŠ¿', 'çœ‹å¥½',
                'ä¹°å…¥', 'æ¨è', 'ä¼˜ç§€', 'é¢†å…ˆ', 'åˆ›æ–°', 'å‘å±•', 'æœºä¼š', 'æ½œåŠ›', 'ç¨³å®š', 'æ”¹å–„',
                'æå‡', 'è¶…é¢„æœŸ', 'ç§¯æ', 'ä¹è§‚', 'å‘å¥½', 'å—ç›Š', 'é¾™å¤´', 'çƒ­ç‚¹', 'çˆ†å‘', 'ç¿»å€',
                'ä¸šç»©', 'å¢æ”¶', 'æ‰©å¼ ', 'åˆä½œ', 'ç­¾çº¦', 'ä¸­æ ‡', 'è·å¾—', 'æˆåŠŸ', 'å®Œæˆ', 'è¾¾æˆ'
            }
            
            negative_words = {
                'ä¸‹è·Œ', 'è·Œåœ', 'åˆ©ç©º', 'ç ´ä½', 'ä¸‹æ»‘', 'äºæŸ', 'é£é™©', 'å›è°ƒ', 'å¼±åŠ¿', 'çœ‹ç©º',
                'å–å‡º', 'å‡æŒ', 'è¾ƒå·®', 'è½å', 'æ»å', 'å›°éš¾', 'å±æœº', 'æ‹…å¿§', 'æ‚²è§‚', 'æ¶åŒ–',
                'ä¸‹é™', 'ä½äºé¢„æœŸ', 'æ¶ˆæ', 'å‹åŠ›', 'å¥—ç‰¢', 'è¢«å¥—', 'æš´è·Œ', 'å´©ç›˜', 'è¸©é›·', 'é€€å¸‚',
                'è¿è§„', 'å¤„ç½š', 'è°ƒæŸ¥', 'åœç‰Œ', 'äºæŸ', 'å€ºåŠ¡', 'è¿çº¦', 'è¯‰è®¼', 'çº çº·', 'é—®é¢˜'
            }
            
            # åˆ†ææ¯ç±»æ–°é—»çš„æƒ…ç»ª
            sentiment_by_type = {}
            overall_scores = []
            
            for text_data in all_texts:
                try:
                    text = text_data['text']
                    text_type = text_data['type']
                    weight = text_data['weight']
                    
                    if not text.strip():
                        continue
                    
                    positive_count = sum(1 for word in positive_words if word in text)
                    negative_count = sum(1 for word in negative_words if word in text)
                    
                    # è®¡ç®—æƒ…ç»ªå¾—åˆ†
                    total_sentiment_words = positive_count + negative_count
                    if total_sentiment_words > 0:
                        sentiment_score = (positive_count - negative_count) / total_sentiment_words
                    else:
                        sentiment_score = -1
                    
                    # åº”ç”¨æƒé‡
                    weighted_score = sentiment_score * weight
                    overall_scores.append(weighted_score)
                    
                    # æŒ‰ç±»å‹ç»Ÿè®¡
                    if text_type not in sentiment_by_type:
                        sentiment_by_type[text_type] = []
                    sentiment_by_type[text_type].append(weighted_score)
                    
                except Exception as e:
                    continue
            
            # è®¡ç®—æ€»ä½“æƒ…ç»ª
            overall_sentiment = sum(overall_scores) / len(overall_scores) if overall_scores else -1
            
            # è®¡ç®—å„ç±»å‹å¹³å‡æƒ…ç»ª
            avg_sentiment_by_type = {}
            for text_type, scores in sentiment_by_type.items():
                avg_sentiment_by_type[text_type] = sum(scores) / len(scores) if scores else -1
            
            # åˆ¤æ–­æƒ…ç»ªè¶‹åŠ¿
            if overall_sentiment > 0.3:
                sentiment_trend = 'éå¸¸ç§¯æ'
            elif overall_sentiment > 0.1:
                sentiment_trend = 'åå‘ç§¯æ'
            elif overall_sentiment > -0.1:
                sentiment_trend = 'ç›¸å¯¹ä¸­æ€§'
            elif overall_sentiment > -0.3:
                sentiment_trend = 'åå‘æ¶ˆæ'
            else:
                sentiment_trend = 'éå¸¸æ¶ˆæ'
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence_score = min(len(all_texts) / 50, 1.0)  # åŸºäºæ–°é—»æ•°é‡çš„ç½®ä¿¡åº¦
            
            result = {
                'overall_sentiment': overall_sentiment,
                'sentiment_by_type': avg_sentiment_by_type,
                'sentiment_trend': sentiment_trend,
                'confidence_score': confidence_score,
                'total_analyzed': len(all_texts),
                'type_distribution': {k: len(v) for k, v in sentiment_by_type.items()},
                'positive_ratio': len([s for s in overall_scores if s > 0]) / len(overall_scores) if overall_scores else 0,
                'negative_ratio': len([s for s in overall_scores if s < 0]) / len(overall_scores) if overall_scores else 0
            }
            
            logger.info(f"âœ“ é«˜çº§æƒ…ç»ªåˆ†æå®Œæˆ: {sentiment_trend} (å¾—åˆ†: {overall_sentiment:.3f})")
            return result
            
        except Exception as e:
            logger.error(f"é«˜çº§æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return {
                'overall_sentiment': 'åˆ†æå¤±è´¥',
                'sentiment_by_type': 'åˆ†æå¤±è´¥',
                'sentiment_trend': 'åˆ†æå¤±è´¥',
                'confidence_score': 'åˆ†æå¤±è´¥',
                'total_analyzed': 'åˆ†æå¤±è´¥'
            }

    def get_stock_name(self, stock_code:str) -> str:
        """è·å–è‚¡ç¥¨åç§°"""
        try:
            stock_info = ak.stock_individual_info_em(symbol=stock_code)
            if not stock_info.empty:
                info_dict = dict(zip(stock_info['item'], stock_info['value']))
                stock_name = info_dict.get('è‚¡ç¥¨ç®€ç§°', stock_code)
                if stock_name and stock_name != stock_code:
                    return stock_name
        except Exception as e:
            logger.warning(f"è·å–è‚¡ç¥¨åç§°å¤±è´¥: {e}")
        
        return stock_code

    def set_streaming_config(self, enabled:bool=True, show_thinking:bool=True):
        """è®¾ç½®æµå¼æ¨ç†é…ç½®"""
        self.config.streaming.enabled = enabled
        self.config.streaming.show_thinking = show_thinking

    def analyze_stock(self, stock_code:str, position_percent:float=0, avg_price:float=-1, enable_streaming:bool=False, streamer:StreamingSender=None):
        """åˆ†æè‚¡ç¥¨çš„ä¸»æ–¹æ³•ï¼ˆä¿®æ­£ç‰ˆï¼Œæ”¯æŒAIæµå¼è¾“å‡ºï¼‰"""
        try:
            logger.info(f"å¼€å§‹å¢å¼ºç‰ˆè‚¡ç¥¨åˆ†æ: {stock_code}")
            if streamer:
                streamer.send_progress('singleProgress', 5, "æ­£åœ¨è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯...")
            
            # è·å–è‚¡ç¥¨åç§°
            stock_name = self.get_stock_name(stock_code)
            
            # è·å–ä»·æ ¼æ•°æ®å’ŒæŠ€æœ¯åˆ†æ
            logger.info("æ­£åœ¨è¿›è¡ŒæŠ€æœ¯åˆ†æ...")
            price_data = self.get_stock_data(stock_code)
            if price_data.empty:
                raise ValueError(f"æ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„ä»·æ ¼æ•°æ®")
            
            price_info = get_price_info(price_data)
            technical_analysis = calculate_technical_indicators(price_data)
            if streamer:
                streamer.send_partial_result({
                    'type': 'basic_info',
                    'stock_code': stock_code,
                    'stock_name': stock_name,
                    'current_price': price_info['current_price'],
                    'price_change': price_info['price_change']
                })
            
            # è·å–è´¢åŠ¡æŒ‡æ ‡å’Œç»¼åˆåŸºæœ¬é¢åˆ†æ
            logger.info("æ­£åœ¨è¿›è¡Œè´¢åŠ¡æŒ‡æ ‡åˆ†æ...")
            fundamental_data = self.get_comprehensive_fundamental_data(stock_code)
            
            # è·å–ç»¼åˆæ–°é—»æ•°æ®å’Œé«˜çº§æƒ…ç»ªåˆ†æ
            logger.info("æ­£åœ¨è¿›è¡Œç»¼åˆæ–°é—»å’Œæƒ…ç»ªåˆ†æ...")
            comprehensive_news_data = self.get_comprehensive_news_data(stock_code, days=30)
            sentiment_analysis = self.calculate_advanced_sentiment_analysis(comprehensive_news_data)
            
            # åˆå¹¶æ–°é—»æ•°æ®åˆ°æƒ…ç»ªåˆ†æç»“æœä¸­ï¼Œæ–¹ä¾¿AIåˆ†æä½¿ç”¨
            sentiment_analysis.update(comprehensive_news_data)
            
            data_quality = {
                'financial_indicators_count': len(fundamental_data.get('financial_indicators', {})),
                'total_news_count': sentiment_analysis.get('total_analyzed', 0),
                'analysis_completeness': 'å®Œæ•´' if len(fundamental_data.get('financial_indicators', {})) >= 15 else 'éƒ¨åˆ†'
            }
            if streamer:
                streamer.send_data_quality(data_quality)
            
            # AIåˆ†æ
            no_thinking_config = analyzer.config.generation.model_copy()
            no_thinking_config.extra_parm = {"chat_template_kwargs": {"enable_thinking": False}}
            if streamer:
                streamer.send_progress('singleProgress', 20, "æ­£åœ¨åˆ†æKçº¿å›¾...")
            _, K_graph_conclusion = k_graph_analysis(stock_name, get_K_graph_table(price_data), no_thinking_config)
            if streamer:
                streamer.send_progress('singleProgress', 40, "æ­£åœ¨åˆ†æç›¸å…³æ–°é—»...")
            _, news_summary = news_summarize(stock_name, sentiment_analysis, no_thinking_config)
            if streamer:
                streamer.send_progress('singleProgress', 60, "æ­£åœ¨åˆ†æå…¬å¸ä»·å€¼...")
            value_prompt, value_analysis = value_analyze(stock_code, stock_name, fundamental_data, price_info, no_thinking_config, streamer)
            
            prompt, ai_analysis = generate_ai_analysis({
                'stock_code': stock_code,
                'stock_name': stock_name,
                'price_info': price_info,
                'technical_analysis': technical_analysis,
                'fundamental_data': fundamental_data,
                "position_percent": position_percent,
                "avg_price": avg_price,
                "news_summary": news_summary,
                "K_graph_conclusion": K_graph_conclusion,
                "value_analysis": value_analysis
            }, analyzer.config.generation, enable_streaming, streamer)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = {
                'stock_code': stock_code,
                'stock_name': stock_name,
                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'price_info': price_info,
                'technical_analysis': technical_analysis,
                'fundamental_data': fundamental_data,
                'comprehensive_news_data': comprehensive_news_data,
                'sentiment_analysis': sentiment_analysis,
                'analysis_weights': self.config.analysis_weights.model_dump(),
                'ai_analysis': ai_analysis,
                'data_quality': data_quality,
                "value_prompt": value_prompt,
                "prompt": prompt
            }
            if streamer:
                streamer.send_progress('singleProgress', 100, "åˆ†æå®Œæˆ")
                streamer.send_final_result(report)
                streamer.send_completion(f"âœ… {stock_code} æµå¼åˆ†æå®Œæˆ")

            logger.info(f"âœ“ å¢å¼ºç‰ˆè‚¡ç¥¨åˆ†æå®Œæˆ: {stock_code}")
            
            return report
            
        except Exception as e:
            logger.error(f"å¢å¼ºç‰ˆè‚¡ç¥¨åˆ†æå¤±è´¥ {stock_code}: {str(e)}")
            raise

    def analyze_stock_with_streaming(self, stock_code:str, position_percent:float=0, avg_price:float=-1, streamer:StreamingSender=None):
        return self.analyze_stock(stock_code, position_percent, avg_price, True, streamer)
    
    def analyze_batch_streaming(self, stock_codes:list[str], client_id:str):
        streamer = StreamingSender(client_id, sse_manager)
        try:
            total_stocks = len(stock_codes)
            streamer.send_log(f"ğŸ“Š å¼€å§‹æµå¼æ‰¹é‡åˆ†æ {total_stocks} åªè‚¡ç¥¨", 'header')
            failed_stocks = []
            for i, stock_code in enumerate(stock_codes):
                try:
                    progress = int((i / total_stocks) * 100)
                    streamer.send_progress('batchProgress', progress, 
                        f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{total_stocks} åªè‚¡ç¥¨", stock_code)
                    
                    report = self.analyze_stock(stock_code)
                    streamer.send_batch_result(i, report)
                    streamer.send_log(f"{stock_code} åˆ†æå®Œæˆ", 'success')
        
                except Exception as e:
                    failed_stocks.append(stock_code)
                    streamer.send_log(f"{stock_code} åˆ†æå¤±è´¥: {e}", 'error')       
        
            streamer.send_progress('batchProgress', 100, f"æ‰¹é‡åˆ†æå®Œæˆ")
            message = f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼æˆåŠŸåˆ†æ {total_stocks - len(failed_stocks)}/{total_stocks} åªè‚¡ç¥¨"
            if failed_stocks:
                message += f"ï¼Œå¤±è´¥: {', '.join(failed_stocks)}"
            streamer.send_completion(message)
            return
        except Exception as e:
            error_msg = f"æ‰¹é‡æµå¼åˆ†æå¤±è´¥: {str(e)}"
            streamer.send_error(error_msg)
            streamer.send_log(f"{error_msg}", 'error')
            streamer.send_completion()
            raise
        
                
    
def init_analyzer(config_path:str) -> WebStockAnalyzer:
    """åˆå§‹åŒ–åˆ†æå™¨"""
    global analyzer
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–WebStockAnalyzer...")
        analyzer = WebStockAnalyzer(config_path)
        logger.info("âœ… WebStockAnalyzeråˆå§‹åŒ–æˆåŠŸ")
        return analyzer
    except Exception as e:
        logger.error(f"âŒ åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None